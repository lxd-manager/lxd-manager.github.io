{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LXD manager The lxd-manager is a management software which is used to orchestrate multiple hosts of lxd containers with a specific deep integration. It was built with very specific demands, but might be useful to someone else. We needed lighweight containers on different kinds of hosts, which behave like physical maschines. Unfortunately with inhomogenious hosts, the lxd built in cluster is not reliable and we barely rely on cluster feature, as most often we want control over where a container is deployed. On top, each container is attached with two network interfaces. eth0 is connected to a bridge of the host and gets an IPv4 NATed address eth1 is bridged directly to the hosts interface and obtains a SLAAC IPv6 address. If required, users can assign IPv4 addresses to this interface through our api. The service contains an authoritative DNS server for all containers which are reachable publicly. Another useful feature is the integration with gitlab not only for user authentication but also for direct provisioning of ssh keys from the user's gitlab profile. Unlike many lxd web UIs, this software uses its own database for persistance and has a background synchronisation service, as live polling of the lxd api is too slow. Actions are usually displayed responsive and then performed in a background task.","title":"Home"},{"location":"#lxd-manager","text":"The lxd-manager is a management software which is used to orchestrate multiple hosts of lxd containers with a specific deep integration. It was built with very specific demands, but might be useful to someone else. We needed lighweight containers on different kinds of hosts, which behave like physical maschines. Unfortunately with inhomogenious hosts, the lxd built in cluster is not reliable and we barely rely on cluster feature, as most often we want control over where a container is deployed. On top, each container is attached with two network interfaces. eth0 is connected to a bridge of the host and gets an IPv4 NATed address eth1 is bridged directly to the hosts interface and obtains a SLAAC IPv6 address. If required, users can assign IPv4 addresses to this interface through our api. The service contains an authoritative DNS server for all containers which are reachable publicly. Another useful feature is the integration with gitlab not only for user authentication but also for direct provisioning of ssh keys from the user's gitlab profile. Unlike many lxd web UIs, this software uses its own database for persistance and has a background synchronisation service, as live polling of the lxd api is too slow. Actions are usually displayed responsive and then performed in a background task.","title":"LXD manager"},{"location":"host-setup/","text":"Host Setup for lxd-manager Interfaces Create a bridge lxdextern with the external ethernet as single port. Assign it a static or DHCP IP address. Storage Create 2 small partitions 50GB at the beginning of each disk. Raid 1 them together and set the mountpoint to / with ext4 . Create 2 large partitions with the rest of the disks. Do not format them. Deploy the machine with a recent ubuntu On Machine Storage pool We use btrfs for the container storage, as it works well with nested containers. The drawbacks are reduced quota control, which is not a factor in our setting. As raid members, use the 2 large partitions of the disks (probably /dev/sd?2 or /dev/sd?3 (if EFI partition available)). mkfs.btrfs -L lxd -d raid1 /dev/sda? /dev/sdb? The filesystem can be verified by any of the participating partitions btrfs filesystem show /dev/sda? Create mount directory mkdir /media/pool and append a line in etc/fstab /dev/sda? /media/pool btrfs user_subvol_rm_allowed 0 0 and mount the fs with mount /media/pool snap Uninstall the apt lxd version sudo apt-get remove --purge lxd lxd-client Install the stable channel from snap. This is a more frequently updated stable release instead of the apt version, which only receives security bug fixes. snap install lxd Kernel Keyring size As the kernel keyring is not namespaced, it needs to be large enough: Add kernel.keys.maxkeys = 5000 to /etc/sysctl.conf and apply it with sysctl -p Time For the distributed database to work, all nodes have to be in sync regarding time. Therefore install apt-get install ntp prepare lxd Set up the cluster master run sudo lxd init , here vs-node5 as example. sudo lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: Name of the storage backend to use (btrfs, ceph, dir, lvm, zfs) [default=zfs]: btrfs Create a new BTRFS pool? (yes/no) [default=yes]: no Name of the existing BTRFS pool or dataset: /media/pool Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: lxdintern What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: none Would you like LXD to be available over the network? (yes/no) [default=no]: yes Address to bind LXD to (not including port) [default=all]: Port to bind LXD to [default=8443]: Trust password for new clients: Again: Would you like stale cached images to be updated automatically? (yes/no) [default=yes] Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: yes config: core.https_address: '[::]:8443' core.trust_password: **** networks: - config: ipv4.address: auto ipv6.address: none description: \"\" managed: false name: lxdintern type: \"\" storage_pools: - config: source: /media/pool description: \"\" name: default driver: btrfs profiles: - config: {} description: \"\" devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic root: path: / pool: default type: disk name: default cluster: null You can check if the cluster is set up correctly with lxc cluster list Profile Add devices to the default profile lxc profile edit default devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic eth1: name: eth1 nictype: bridged parent: lxdextern type: nic root: path: / pool: local type: disk The lxdintern bridge must be attached to eth0, as the DHCP will query on this interface. This assures that the containers do not query for a DHCP IP on the bridged external bridge, but only fetch an IPv6 through this bridge. Test You can create a test container with lxc launch ubuntu:18.04 test and check it with lxc list Adding the host to the managment authenticate with a superuser to the api Subnet add the subnet the host resides in. Host add a new host with a name, subnet and url as well as the trust pw. Troubleshooting Permission denied to mount /media/pool/containers needs 711 permissions Adding host fails Set the LXD_CA_CERT environment variable to false, if your lxd-host uses a self signed certificate.","title":"Host-Setup"},{"location":"host-setup/#host-setup-for-lxd-manager","text":"","title":"Host Setup for lxd-manager"},{"location":"host-setup/#interfaces","text":"Create a bridge lxdextern with the external ethernet as single port. Assign it a static or DHCP IP address.","title":"Interfaces"},{"location":"host-setup/#storage","text":"Create 2 small partitions 50GB at the beginning of each disk. Raid 1 them together and set the mountpoint to / with ext4 . Create 2 large partitions with the rest of the disks. Do not format them.","title":"Storage"},{"location":"host-setup/#deploy-the-machine","text":"with a recent ubuntu","title":"Deploy the machine"},{"location":"host-setup/#on-machine","text":"","title":"On Machine"},{"location":"host-setup/#storage-pool","text":"We use btrfs for the container storage, as it works well with nested containers. The drawbacks are reduced quota control, which is not a factor in our setting. As raid members, use the 2 large partitions of the disks (probably /dev/sd?2 or /dev/sd?3 (if EFI partition available)). mkfs.btrfs -L lxd -d raid1 /dev/sda? /dev/sdb? The filesystem can be verified by any of the participating partitions btrfs filesystem show /dev/sda? Create mount directory mkdir /media/pool and append a line in etc/fstab /dev/sda? /media/pool btrfs user_subvol_rm_allowed 0 0 and mount the fs with mount /media/pool","title":"Storage pool"},{"location":"host-setup/#snap","text":"Uninstall the apt lxd version sudo apt-get remove --purge lxd lxd-client Install the stable channel from snap. This is a more frequently updated stable release instead of the apt version, which only receives security bug fixes. snap install lxd","title":"snap"},{"location":"host-setup/#kernel-keyring-size","text":"As the kernel keyring is not namespaced, it needs to be large enough: Add kernel.keys.maxkeys = 5000 to /etc/sysctl.conf and apply it with sysctl -p","title":"Kernel Keyring size"},{"location":"host-setup/#time","text":"For the distributed database to work, all nodes have to be in sync regarding time. Therefore install apt-get install ntp","title":"Time"},{"location":"host-setup/#prepare-lxd","text":"Set up the cluster master run sudo lxd init , here vs-node5 as example. sudo lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: Name of the storage backend to use (btrfs, ceph, dir, lvm, zfs) [default=zfs]: btrfs Create a new BTRFS pool? (yes/no) [default=yes]: no Name of the existing BTRFS pool or dataset: /media/pool Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: lxdintern What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: none Would you like LXD to be available over the network? (yes/no) [default=no]: yes Address to bind LXD to (not including port) [default=all]: Port to bind LXD to [default=8443]: Trust password for new clients: Again: Would you like stale cached images to be updated automatically? (yes/no) [default=yes] Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: yes config: core.https_address: '[::]:8443' core.trust_password: **** networks: - config: ipv4.address: auto ipv6.address: none description: \"\" managed: false name: lxdintern type: \"\" storage_pools: - config: source: /media/pool description: \"\" name: default driver: btrfs profiles: - config: {} description: \"\" devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic root: path: / pool: default type: disk name: default cluster: null You can check if the cluster is set up correctly with lxc cluster list","title":"prepare lxd"},{"location":"host-setup/#profile","text":"Add devices to the default profile lxc profile edit default devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic eth1: name: eth1 nictype: bridged parent: lxdextern type: nic root: path: / pool: local type: disk The lxdintern bridge must be attached to eth0, as the DHCP will query on this interface. This assures that the containers do not query for a DHCP IP on the bridged external bridge, but only fetch an IPv6 through this bridge.","title":"Profile"},{"location":"host-setup/#test","text":"You can create a test container with lxc launch ubuntu:18.04 test and check it with lxc list","title":"Test"},{"location":"host-setup/#adding-the-host-to-the-managment","text":"authenticate with a superuser to the api","title":"Adding the host to the managment"},{"location":"host-setup/#subnet","text":"add the subnet the host resides in.","title":"Subnet"},{"location":"host-setup/#host","text":"add a new host with a name, subnet and url as well as the trust pw.","title":"Host"},{"location":"host-setup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"host-setup/#permission-denied-to-mount","text":"/media/pool/containers needs 711 permissions","title":"Permission denied to mount"},{"location":"host-setup/#adding-host-fails","text":"Set the LXD_CA_CERT environment variable to false, if your lxd-host uses a self signed certificate.","title":"Adding host fails"},{"location":"install/","text":"LXD manager deployment This is the repository which helps with the deployment of the lxd-manager. It is up to you to run all the required services on your own, but as it is rather complex, we help you with the following instructions. Requirements The services are all dockerised and require a linux host with the following software docker docker-compose Step-by-step guide Repo Clone this repository: git clone https://github.com/lxd-manager/deploy.git We suggest you create a branch with your actual configuration values. Web Proxy This docker-compose enables a reverse proxy for handling TLS. If you do NOT want to use this or provide you own ingress service, please remove the reverse-proxy section from the services defined in docker-compose.yml and provide a mean to access the nginx service on port 80. Otherwise create an external network with: docker network create web Decide on the FQDN where you will be hosting the webservice and replace service-fqdn at the docker-compose.yml file nginx service in the traefik labels. The proxy automatically generates Let's Encrypt TLS vertificates for the domain. For this, your email is required in the [certificatesResolvers.le.acme] section of the traefik.toml file. LXD connection The lxd api uses TLS client certificates as authentication mechanism. Therefore place a TLS key and (optionally self signed) certificate to certs/lxd.key and certs/lxd.crt . You add new hosts through the api, where you supply a trust password which is used to establish the certificate at the new host. openssl req -x509 -newkey rsa:4096 -nodes -keyout certs/lxd.key -out certs/lxd.crt -days 3650 There is an option to provide the CA certificate the server is using via the LXD_CA_CERT environment variable. If the lxd server uses the default self signed certificate, you have to set the variable to \"False\" DNS There is a DNS server which generates responses based on the containers available in the database. This requires you to set up a NS delegation of the subdomain ct-subdomain.d.tld. under which the containers will be named to a nameserver ns-fqdn.d.tld. which points to your deployment. Replace server-ip in the ports section with the external IP where the DNS server should listen. (If it is an IPv6 address omit [ and ]) Example In your DNS zone set: ct.example.org. 1800 IN NS lxd-ui.example.org. lxd-ui.example.org. 1800 IN A 1.2.3.4 and in docker-compose.yml set ns-fqdn.d.tld. = lxd-ui.example.org. ct-subdomain.d.tld. = ct.example.org. Your containers get FQDNs of the form name.ct.example.net Secrets The services require credentials to communicate. For this purpose, please create the following files unter secrets/ : secrets/ct_postgres with 50 random alphanumeric characters (used as Database password of the ctapi user) secrets/django_secret with 50 random alphanumeric characters (used as django session secret) secrets/db_encrypt with 50 random alphanumeric characters (used to symmetrically encrypt the ssh host keys of the containers) secrets/gitlab_id the id of the oauth application from gitlab secrets/gitlab_secret the secret ot the oauth application The random passwords may be created by head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/ct_postgres head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/django_secret head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/db_encrypt Gitlab OAuth To get the gitlab_id and _secret, create an gitlab application with the following parameters: callback url: https://ui-fqdn/social-auth/complete/gitlab/ not trusted confidential (_id and _secret remains at the server) scopes: read_user set the gitlab url for the services beat, api, celery environments: SOCIAL_AUTH_GITLAB_API_URL: \"https://gitlab.yourdomain.com\" Run docker-compose pull docker-compose up -d to create an admin user, execute docker-compose exec api python3 manage.py createsuperuser Backup To get clean backups, run the backup.sh before a system snapshot to dump the database into the backup folder.","title":"Installation"},{"location":"install/#lxd-manager-deployment","text":"This is the repository which helps with the deployment of the lxd-manager. It is up to you to run all the required services on your own, but as it is rather complex, we help you with the following instructions.","title":"LXD manager deployment"},{"location":"install/#requirements","text":"The services are all dockerised and require a linux host with the following software docker docker-compose","title":"Requirements"},{"location":"install/#step-by-step-guide","text":"","title":"Step-by-step guide"},{"location":"install/#repo","text":"Clone this repository: git clone https://github.com/lxd-manager/deploy.git We suggest you create a branch with your actual configuration values.","title":"Repo"},{"location":"install/#web-proxy","text":"This docker-compose enables a reverse proxy for handling TLS. If you do NOT want to use this or provide you own ingress service, please remove the reverse-proxy section from the services defined in docker-compose.yml and provide a mean to access the nginx service on port 80. Otherwise create an external network with: docker network create web Decide on the FQDN where you will be hosting the webservice and replace service-fqdn at the docker-compose.yml file nginx service in the traefik labels. The proxy automatically generates Let's Encrypt TLS vertificates for the domain. For this, your email is required in the [certificatesResolvers.le.acme] section of the traefik.toml file.","title":"Web Proxy"},{"location":"install/#lxd-connection","text":"The lxd api uses TLS client certificates as authentication mechanism. Therefore place a TLS key and (optionally self signed) certificate to certs/lxd.key and certs/lxd.crt . You add new hosts through the api, where you supply a trust password which is used to establish the certificate at the new host. openssl req -x509 -newkey rsa:4096 -nodes -keyout certs/lxd.key -out certs/lxd.crt -days 3650 There is an option to provide the CA certificate the server is using via the LXD_CA_CERT environment variable. If the lxd server uses the default self signed certificate, you have to set the variable to \"False\"","title":"LXD connection"},{"location":"install/#dns","text":"There is a DNS server which generates responses based on the containers available in the database. This requires you to set up a NS delegation of the subdomain ct-subdomain.d.tld. under which the containers will be named to a nameserver ns-fqdn.d.tld. which points to your deployment. Replace server-ip in the ports section with the external IP where the DNS server should listen. (If it is an IPv6 address omit [ and ])","title":"DNS"},{"location":"install/#example","text":"In your DNS zone set: ct.example.org. 1800 IN NS lxd-ui.example.org. lxd-ui.example.org. 1800 IN A 1.2.3.4 and in docker-compose.yml set ns-fqdn.d.tld. = lxd-ui.example.org. ct-subdomain.d.tld. = ct.example.org. Your containers get FQDNs of the form name.ct.example.net","title":"Example"},{"location":"install/#secrets","text":"The services require credentials to communicate. For this purpose, please create the following files unter secrets/ : secrets/ct_postgres with 50 random alphanumeric characters (used as Database password of the ctapi user) secrets/django_secret with 50 random alphanumeric characters (used as django session secret) secrets/db_encrypt with 50 random alphanumeric characters (used to symmetrically encrypt the ssh host keys of the containers) secrets/gitlab_id the id of the oauth application from gitlab secrets/gitlab_secret the secret ot the oauth application The random passwords may be created by head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/ct_postgres head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/django_secret head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/db_encrypt","title":"Secrets"},{"location":"install/#gitlab-oauth","text":"To get the gitlab_id and _secret, create an gitlab application with the following parameters: callback url: https://ui-fqdn/social-auth/complete/gitlab/ not trusted confidential (_id and _secret remains at the server) scopes: read_user set the gitlab url for the services beat, api, celery environments: SOCIAL_AUTH_GITLAB_API_URL: \"https://gitlab.yourdomain.com\"","title":"Gitlab OAuth"},{"location":"install/#run","text":"docker-compose pull docker-compose up -d to create an admin user, execute docker-compose exec api python3 manage.py createsuperuser","title":"Run"},{"location":"install/#backup","text":"To get clean backups, run the backup.sh before a system snapshot to dump the database into the backup folder.","title":"Backup"}]}