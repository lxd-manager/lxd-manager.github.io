{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LXD manager The lxd-manager is a management software which is used to orchestrate multiple hosts of lxd containers with a specific deep integration. It was built with very specific demands, but might be useful to someone else. We needed lighweight containers on different kinds of hosts, which behave like physical maschines. Unfortunately with inhomogenious hosts, the lxd built in cluster is not reliable and we barely rely on cluster feature, as most often we want control over where a container is deployed. On top, each container is attached with two network interfaces. eth0 is connected to a bridge of the host and gets an IPv4 NATed address eth1 is bridged directly to the hosts interface and obtains a SLAAC IPv6 address. If required, users can assign IPv4 addresses to this interface through our api. The service contains an authoritative DNS server for all containers which are reachable publicly. Another useful feature is the integration with gitlab not only for user authentication but also for direct provisioning of ssh keys from the user's gitlab profile. Unlike many lxd web UIs, this software uses its own database for persistance and has a background synchronisation service, as live polling of the lxd api is too slow. Actions are usually displayed responsive and then performed in a background task.","title":"Home"},{"location":"#lxd-manager","text":"The lxd-manager is a management software which is used to orchestrate multiple hosts of lxd containers with a specific deep integration. It was built with very specific demands, but might be useful to someone else. We needed lighweight containers on different kinds of hosts, which behave like physical maschines. Unfortunately with inhomogenious hosts, the lxd built in cluster is not reliable and we barely rely on cluster feature, as most often we want control over where a container is deployed. On top, each container is attached with two network interfaces. eth0 is connected to a bridge of the host and gets an IPv4 NATed address eth1 is bridged directly to the hosts interface and obtains a SLAAC IPv6 address. If required, users can assign IPv4 addresses to this interface through our api. The service contains an authoritative DNS server for all containers which are reachable publicly. Another useful feature is the integration with gitlab not only for user authentication but also for direct provisioning of ssh keys from the user's gitlab profile. Unlike many lxd web UIs, this software uses its own database for persistance and has a background synchronisation service, as live polling of the lxd api is too slow. Actions are usually displayed responsive and then performed in a background task.","title":"LXD manager"},{"location":"containers/","text":"Basic Container operations The general creation and modification of containers is hopefully intuitive. List your containers You get a list of all containers you have access to in a listing overview. You can search for arbitrary fields to limit the list. Light gray IPs are either assigned, but not present on the container or present on the container without reservation in the dashboard. A restart resolves most non-malicious situations. Create a container To create a new container, klick the green plus button on the bottom right. Please decide if you want to run another container solution such as docker inside the container. In this case, mark the Nesting checkbox. After creation, the container is in a stopped state. Start the container and it will acquire an IP and deploy your Gitlab SSH keys to the default user. Soon the hostname of your container will be resolvable and you can connect to the new instance by e.g. ssh ubuntu@testct.cts.domain.tld Edit a container All container can be edited by klicking on the green cog on the left of the name. This modal allows you then to change some parameters of the container and add or delete legacy IPs. Adding or removing an IP requires either to restart the container or you have to add it manually inside the container. If the project changes of there the users of the project changed, you can redeploy keys to add the new keys to the container for ssh access. \u26a0\ufe0f existing SSH keys will not be deleted","title":"Containers"},{"location":"containers/#basic-container-operations","text":"The general creation and modification of containers is hopefully intuitive.","title":"Basic Container operations"},{"location":"containers/#list-your-containers","text":"You get a list of all containers you have access to in a listing overview. You can search for arbitrary fields to limit the list. Light gray IPs are either assigned, but not present on the container or present on the container without reservation in the dashboard. A restart resolves most non-malicious situations.","title":"List your containers"},{"location":"containers/#create-a-container","text":"To create a new container, klick the green plus button on the bottom right. Please decide if you want to run another container solution such as docker inside the container. In this case, mark the Nesting checkbox. After creation, the container is in a stopped state. Start the container and it will acquire an IP and deploy your Gitlab SSH keys to the default user. Soon the hostname of your container will be resolvable and you can connect to the new instance by e.g. ssh ubuntu@testct.cts.domain.tld","title":"Create a container"},{"location":"containers/#edit-a-container","text":"All container can be edited by klicking on the green cog on the left of the name. This modal allows you then to change some parameters of the container and add or delete legacy IPs. Adding or removing an IP requires either to restart the container or you have to add it manually inside the container. If the project changes of there the users of the project changed, you can redeploy keys to add the new keys to the container for ssh access. \u26a0\ufe0f existing SSH keys will not be deleted","title":"Edit a container"},{"location":"host-setup/","text":"Host Setup for lxd-manager Interfaces Create a bridge lxdextern with the external ethernet as single port. Assign it a static or DHCP IP address. Storage Create 2 small partitions 50GB at the beginning of each disk. Raid 1 them together and set the mountpoint to / with ext4 . Create 2 large partitions with the rest of the disks. Do not format them. Deploy the machine with a recent ubuntu On Machine Storage pool We use btrfs for the container storage, as it works well with nested containers. The drawbacks are reduced quota control, which is not a factor in our setting. As raid members, use the 2 large partitions of the disks (probably /dev/sd?2 or /dev/sd?3 (if EFI partition available)). mkfs.btrfs -L lxd -d raid1 /dev/sda? /dev/sdb? The filesystem can be verified by any of the participating partitions btrfs filesystem show /dev/sda? Create mount directory mkdir /media/pool and append a line in etc/fstab /dev/sda? /media/pool btrfs user_subvol_rm_allowed 0 0 and mount the fs with mount /media/pool snap Uninstall the apt lxd version sudo apt-get remove --purge lxd lxd-client Install the stable channel from snap. This is a more frequently updated stable release instead of the apt version, which only receives security bug fixes. snap install lxd Kernel Keyring size As the kernel keyring is not namespaced, it needs to be large enough: Add kernel.keys.maxkeys = 5000 to /etc/sysctl.conf and apply it with sysctl -p Time For the distributed database to work, all nodes have to be in sync regarding time. Therefore install apt-get install ntp prepare lxd Set up the cluster master run sudo lxd init , here vs-node5 as example. sudo lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: Name of the storage backend to use (btrfs, ceph, dir, lvm, zfs) [default=zfs]: btrfs Create a new BTRFS pool? (yes/no) [default=yes]: no Name of the existing BTRFS pool or dataset: /media/pool Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: lxdintern What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: none Would you like LXD to be available over the network? (yes/no) [default=no]: yes Address to bind LXD to (not including port) [default=all]: Port to bind LXD to [default=8443]: Trust password for new clients: Again: Would you like stale cached images to be updated automatically? (yes/no) [default=yes] Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: yes config: core.https_address: '[::]:8443' core.trust_password: **** networks: - config: ipv4.address: auto ipv6.address: none description: \"\" managed: false name: lxdintern type: \"\" storage_pools: - config: source: /media/pool description: \"\" name: default driver: btrfs profiles: - config: {} description: \"\" devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic root: path: / pool: default type: disk name: default cluster: null You can check if the cluster is set up correctly with lxc cluster list Profile Add devices to the default profile lxc profile edit default devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic eth1: name: eth1 nictype: bridged parent: lxdextern type: nic root: path: / pool: local type: disk The lxdintern bridge must be attached to eth0, as the DHCP will query on this interface. This assures that the containers do not query for a DHCP IP on the bridged external bridge, but only fetch an IPv6 through this bridge. Test You can create a test container with lxc launch ubuntu:18.04 test and check it with lxc list Adding the host to the managment authenticate with a superuser to the api Subnet add the subnet the host resides in. Host add a new host with a name, subnet and url as well as the trust pw. Troubleshooting Permission denied to mount /media/pool/containers needs 711 permissions Adding host fails Set the LXD_CA_CERT environment variable to false, if your lxd-host uses a self signed certificate.","title":"Host-Setup"},{"location":"host-setup/#host-setup-for-lxd-manager","text":"","title":"Host Setup for lxd-manager"},{"location":"host-setup/#interfaces","text":"Create a bridge lxdextern with the external ethernet as single port. Assign it a static or DHCP IP address.","title":"Interfaces"},{"location":"host-setup/#storage","text":"Create 2 small partitions 50GB at the beginning of each disk. Raid 1 them together and set the mountpoint to / with ext4 . Create 2 large partitions with the rest of the disks. Do not format them.","title":"Storage"},{"location":"host-setup/#deploy-the-machine","text":"with a recent ubuntu","title":"Deploy the machine"},{"location":"host-setup/#on-machine","text":"","title":"On Machine"},{"location":"host-setup/#storage-pool","text":"We use btrfs for the container storage, as it works well with nested containers. The drawbacks are reduced quota control, which is not a factor in our setting. As raid members, use the 2 large partitions of the disks (probably /dev/sd?2 or /dev/sd?3 (if EFI partition available)). mkfs.btrfs -L lxd -d raid1 /dev/sda? /dev/sdb? The filesystem can be verified by any of the participating partitions btrfs filesystem show /dev/sda? Create mount directory mkdir /media/pool and append a line in etc/fstab /dev/sda? /media/pool btrfs user_subvol_rm_allowed 0 0 and mount the fs with mount /media/pool","title":"Storage pool"},{"location":"host-setup/#snap","text":"Uninstall the apt lxd version sudo apt-get remove --purge lxd lxd-client Install the stable channel from snap. This is a more frequently updated stable release instead of the apt version, which only receives security bug fixes. snap install lxd","title":"snap"},{"location":"host-setup/#kernel-keyring-size","text":"As the kernel keyring is not namespaced, it needs to be large enough: Add kernel.keys.maxkeys = 5000 to /etc/sysctl.conf and apply it with sysctl -p","title":"Kernel Keyring size"},{"location":"host-setup/#time","text":"For the distributed database to work, all nodes have to be in sync regarding time. Therefore install apt-get install ntp","title":"Time"},{"location":"host-setup/#prepare-lxd","text":"Set up the cluster master run sudo lxd init , here vs-node5 as example. sudo lxd init Would you like to use LXD clustering? (yes/no) [default=no]: Do you want to configure a new storage pool? (yes/no) [default=yes]: Name of the new storage pool [default=default]: Name of the storage backend to use (btrfs, ceph, dir, lvm, zfs) [default=zfs]: btrfs Create a new BTRFS pool? (yes/no) [default=yes]: no Name of the existing BTRFS pool or dataset: /media/pool Would you like to connect to a MAAS server? (yes/no) [default=no]: Would you like to create a new local network bridge? (yes/no) [default=yes]: What should the new bridge be called? [default=lxdbr0]: lxdintern What IPv4 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: What IPv6 address should be used? (CIDR subnet notation, \u201cauto\u201d or \u201cnone\u201d) [default=auto]: none Would you like LXD to be available over the network? (yes/no) [default=no]: yes Address to bind LXD to (not including port) [default=all]: Port to bind LXD to [default=8443]: Trust password for new clients: Again: Would you like stale cached images to be updated automatically? (yes/no) [default=yes] Would you like a YAML \"lxd init\" preseed to be printed? (yes/no) [default=no]: yes config: core.https_address: '[::]:8443' core.trust_password: **** networks: - config: ipv4.address: auto ipv6.address: none description: \"\" managed: false name: lxdintern type: \"\" storage_pools: - config: source: /media/pool description: \"\" name: default driver: btrfs profiles: - config: {} description: \"\" devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic root: path: / pool: default type: disk name: default cluster: null You can check if the cluster is set up correctly with lxc cluster list","title":"prepare lxd"},{"location":"host-setup/#profile","text":"Add devices to the default profile lxc profile edit default devices: eth0: name: eth0 nictype: bridged parent: lxdintern type: nic eth1: name: eth1 nictype: bridged parent: lxdextern type: nic root: path: / pool: local type: disk The lxdintern bridge must be attached to eth0, as the DHCP will query on this interface. This assures that the containers do not query for a DHCP IP on the bridged external bridge, but only fetch an IPv6 through this bridge.","title":"Profile"},{"location":"host-setup/#test","text":"You can create a test container with lxc launch ubuntu:18.04 test and check it with lxc list","title":"Test"},{"location":"host-setup/#adding-the-host-to-the-managment","text":"authenticate with a superuser to the api","title":"Adding the host to the managment"},{"location":"host-setup/#subnet","text":"add the subnet the host resides in.","title":"Subnet"},{"location":"host-setup/#host","text":"add a new host with a name, subnet and url as well as the trust pw.","title":"Host"},{"location":"host-setup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"host-setup/#permission-denied-to-mount","text":"/media/pool/containers needs 711 permissions","title":"Permission denied to mount"},{"location":"host-setup/#adding-host-fails","text":"Set the LXD_CA_CERT environment variable to false, if your lxd-host uses a self signed certificate.","title":"Adding host fails"},{"location":"images/","text":"Images Contrary to an LXD managed cluster, you have to specify which images should be available on the hosts. To add a new image to the list of images to be synced, create an image object at /api/image/ with e.g. \"sync\": true, \"server\": \"https://cloud-images.ubuntu.com/releases\", \"protocol\": \"simplestreams\", \"alias\": \"x\" Make sure that the images have a working cloud-init installed, as this is necessary for automatic provisioning","title":"Images"},{"location":"images/#images","text":"Contrary to an LXD managed cluster, you have to specify which images should be available on the hosts. To add a new image to the list of images to be synced, create an image object at /api/image/ with e.g. \"sync\": true, \"server\": \"https://cloud-images.ubuntu.com/releases\", \"protocol\": \"simplestreams\", \"alias\": \"x\" Make sure that the images have a working cloud-init installed, as this is necessary for automatic provisioning","title":"Images"},{"location":"install/","text":"LXD manager deployment This is the repository which helps with the deployment of the lxd-manager. It is up to you to run all the required services on your own, but as it is rather complex, we help you with the following instructions. Requirements The services are all dockerised and require a linux host with the following software docker docker-compose Step-by-step guide Repo Clone this repository: git clone https://github.com/lxd-manager/deploy.git We suggest you create a branch with your actual configuration values. Web Proxy This docker-compose enables a reverse proxy for handling TLS. If you do NOT want to use this or provide you own ingress service, please remove the reverse-proxy section from the services defined in docker-compose.yml and provide a mean to access the nginx service on port 80. Otherwise create an external network with: docker network create web Decide on the FQDN where you will be hosting the webservice and replace service-fqdn at the docker-compose.yml file nginx service in the traefik labels. The proxy automatically generates Let's Encrypt TLS vertificates for the domain. For this, your email is required in the [certificatesResolvers.le.acme] section of the traefik.toml file. LXD connection The lxd api uses TLS client certificates as authentication mechanism. Therefore place a TLS key and (optionally self signed) certificate to certs/lxd.key and certs/lxd.crt . You add new hosts through the api, where you supply a trust password which is used to establish the certificate at the new host. openssl req -x509 -newkey rsa:4096 -nodes -keyout certs/lxd.key -out certs/lxd.crt -days 3650 There is an option to provide the CA certificate the server is using via the LXD_CA_CERT environment variable. If the lxd server uses the default self signed certificate, you have to set the variable to \"False\" DNS There is a DNS server which generates responses based on the containers available in the database. This requires you to set up a NS delegation of the subdomain ct-subdomain.d.tld. under which the containers will be named to a nameserver ns-fqdn.d.tld. which points to your deployment. Replace server-ip in the ports section with the external IP where the DNS server should listen. (If it is an IPv6 address omit [ and ]) Example In your DNS zone set: ct.example.org. 1800 IN NS lxd-ui.example.org. lxd-ui.example.org. 1800 IN A 1.2.3.4 and in docker-compose.yml set ns-fqdn.d.tld. = lxd-ui.example.org. ct-subdomain.d.tld. = ct.example.org. Your containers get FQDNs of the form name.ct.example.net Secrets The services require credentials to communicate. For this purpose, please create the following files unter secrets/ : secrets/ct_postgres with 50 random alphanumeric characters (used as Database password of the ctapi user) secrets/django_secret with 50 random alphanumeric characters (used as django session secret) secrets/db_encrypt with 50 random alphanumeric characters (used to symmetrically encrypt the ssh host keys of the containers) secrets/gitlab_id the id of the oauth application from gitlab secrets/gitlab_secret the secret ot the oauth application The random passwords may be created by head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/ct_postgres head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/django_secret head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/db_encrypt Gitlab OAuth To get the gitlab_id and _secret, create an gitlab application with the following parameters: callback url: https://ui-fqdn/social-auth/complete/gitlab/ not trusted confidential (_id and _secret remains at the server) scopes: read_user set the gitlab url for the services beat, api, celery environments: SOCIAL_AUTH_GITLAB_API_URL: \"https://gitlab.yourdomain.com\" As oauth provides authentication but no authorization, an administrator has to grant new users the required permissions after they first linked their oauth identity. This is performed by granting them the Can view container permission at /admin/auth/user/ Run docker-compose pull docker-compose up -d to create an admin user, execute docker-compose exec api python3 manage.py createsuperuser Backup To get clean backups, run the backup.sh before a system snapshot to dump the database into the backup folder.","title":"Installation"},{"location":"install/#lxd-manager-deployment","text":"This is the repository which helps with the deployment of the lxd-manager. It is up to you to run all the required services on your own, but as it is rather complex, we help you with the following instructions.","title":"LXD manager deployment"},{"location":"install/#requirements","text":"The services are all dockerised and require a linux host with the following software docker docker-compose","title":"Requirements"},{"location":"install/#step-by-step-guide","text":"","title":"Step-by-step guide"},{"location":"install/#repo","text":"Clone this repository: git clone https://github.com/lxd-manager/deploy.git We suggest you create a branch with your actual configuration values.","title":"Repo"},{"location":"install/#web-proxy","text":"This docker-compose enables a reverse proxy for handling TLS. If you do NOT want to use this or provide you own ingress service, please remove the reverse-proxy section from the services defined in docker-compose.yml and provide a mean to access the nginx service on port 80. Otherwise create an external network with: docker network create web Decide on the FQDN where you will be hosting the webservice and replace service-fqdn at the docker-compose.yml file nginx service in the traefik labels. The proxy automatically generates Let's Encrypt TLS vertificates for the domain. For this, your email is required in the [certificatesResolvers.le.acme] section of the traefik.toml file.","title":"Web Proxy"},{"location":"install/#lxd-connection","text":"The lxd api uses TLS client certificates as authentication mechanism. Therefore place a TLS key and (optionally self signed) certificate to certs/lxd.key and certs/lxd.crt . You add new hosts through the api, where you supply a trust password which is used to establish the certificate at the new host. openssl req -x509 -newkey rsa:4096 -nodes -keyout certs/lxd.key -out certs/lxd.crt -days 3650 There is an option to provide the CA certificate the server is using via the LXD_CA_CERT environment variable. If the lxd server uses the default self signed certificate, you have to set the variable to \"False\"","title":"LXD connection"},{"location":"install/#dns","text":"There is a DNS server which generates responses based on the containers available in the database. This requires you to set up a NS delegation of the subdomain ct-subdomain.d.tld. under which the containers will be named to a nameserver ns-fqdn.d.tld. which points to your deployment. Replace server-ip in the ports section with the external IP where the DNS server should listen. (If it is an IPv6 address omit [ and ])","title":"DNS"},{"location":"install/#example","text":"In your DNS zone set: ct.example.org. 1800 IN NS lxd-ui.example.org. lxd-ui.example.org. 1800 IN A 1.2.3.4 and in docker-compose.yml set ns-fqdn.d.tld. = lxd-ui.example.org. ct-subdomain.d.tld. = ct.example.org. Your containers get FQDNs of the form name.ct.example.net","title":"Example"},{"location":"install/#secrets","text":"The services require credentials to communicate. For this purpose, please create the following files unter secrets/ : secrets/ct_postgres with 50 random alphanumeric characters (used as Database password of the ctapi user) secrets/django_secret with 50 random alphanumeric characters (used as django session secret) secrets/db_encrypt with 50 random alphanumeric characters (used to symmetrically encrypt the ssh host keys of the containers) secrets/gitlab_id the id of the oauth application from gitlab secrets/gitlab_secret the secret ot the oauth application The random passwords may be created by head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/ct_postgres head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/django_secret head /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 > secrets/db_encrypt","title":"Secrets"},{"location":"install/#gitlab-oauth","text":"To get the gitlab_id and _secret, create an gitlab application with the following parameters: callback url: https://ui-fqdn/social-auth/complete/gitlab/ not trusted confidential (_id and _secret remains at the server) scopes: read_user set the gitlab url for the services beat, api, celery environments: SOCIAL_AUTH_GITLAB_API_URL: \"https://gitlab.yourdomain.com\" As oauth provides authentication but no authorization, an administrator has to grant new users the required permissions after they first linked their oauth identity. This is performed by granting them the Can view container permission at /admin/auth/user/","title":"Gitlab OAuth"},{"location":"install/#run","text":"docker-compose pull docker-compose up -d to create an admin user, execute docker-compose exec api python3 manage.py createsuperuser","title":"Run"},{"location":"install/#backup","text":"To get clean backups, run the backup.sh before a system snapshot to dump the database into the backup folder.","title":"Backup"},{"location":"migration/","text":"Migrations Providing seamless upgrades and migration to a new system is always preferable. Import from any lxd instance The LXD daemons support copying of containers between each other. Conenct lxd instances Therefore add one of your new hosts to the remotes of the old host, such that it is listed in lxc remote ls as e.g. new-lxd Preprare containers While the container is still running on the old host: Extract the ssh host keys To preserve the ssh host keys and keep the fingerprint: host$ lxc exec <ctname> bash container$ for i in /etc/ssh/ssh_host_*; do echo $i; cat $i; echo ''; done Keep the output of the loop in a safe place, as they can be used to impersonate the container Remove Profiles If your container has profiles apart from the default profile, remove them by lxc config edit <ctname> Stop container Live migration of containers is possible, but not recommended. lxc stop <ctname> Copy Container lxc copy <ctname> new-lxd: Integration Host Keys Wait until the background synchronisation registered the new container. Once you know the ID, go to https://your-manager.tld/api/container/< ID >/import_keys/ where you can paste the output of the host keys command. This allows to use the same host keys as before, but they are now managed by the application. Project Assign the container to a project to grant access to non-admin users at https://your-manager.tld/api/container/< ID >/ You may then redeploy the keys to fully integrate the instance into lxd-manager. IPs If your container possessed special IPs, please add then through the UI and restart your container.","title":"Migration"},{"location":"migration/#migrations","text":"Providing seamless upgrades and migration to a new system is always preferable.","title":"Migrations"},{"location":"migration/#import-from-any-lxd-instance","text":"The LXD daemons support copying of containers between each other.","title":"Import from any lxd instance"},{"location":"migration/#conenct-lxd-instances","text":"Therefore add one of your new hosts to the remotes of the old host, such that it is listed in lxc remote ls as e.g. new-lxd","title":"Conenct lxd instances"},{"location":"migration/#preprare-containers","text":"While the container is still running on the old host:","title":"Preprare containers"},{"location":"migration/#extract-the-ssh-host-keys","text":"To preserve the ssh host keys and keep the fingerprint: host$ lxc exec <ctname> bash container$ for i in /etc/ssh/ssh_host_*; do echo $i; cat $i; echo ''; done Keep the output of the loop in a safe place, as they can be used to impersonate the container","title":"Extract the ssh host keys"},{"location":"migration/#remove-profiles","text":"If your container has profiles apart from the default profile, remove them by lxc config edit <ctname>","title":"Remove Profiles"},{"location":"migration/#stop-container","text":"Live migration of containers is possible, but not recommended. lxc stop <ctname>","title":"Stop container"},{"location":"migration/#copy-container","text":"lxc copy <ctname> new-lxd:","title":"Copy Container"},{"location":"migration/#integration","text":"","title":"Integration"},{"location":"migration/#host-keys","text":"Wait until the background synchronisation registered the new container. Once you know the ID, go to https://your-manager.tld/api/container/< ID >/import_keys/ where you can paste the output of the host keys command. This allows to use the same host keys as before, but they are now managed by the application.","title":"Host Keys"},{"location":"migration/#project","text":"Assign the container to a project to grant access to non-admin users at https://your-manager.tld/api/container/< ID >/ You may then redeploy the keys to fully integrate the instance into lxd-manager.","title":"Project"},{"location":"migration/#ips","text":"If your container possessed special IPs, please add then through the UI and restart your container.","title":"IPs"},{"location":"monitoring/","text":"Monitoring The hosts for the containers are vital to the whole system. Therefore it is suggested to monitor them. This helps with placement of new containers too. A solid approach is to use the prometheus node exporter as explained below Prometheus Grafana Stack Please deploy a Prometheus Grafana stack. Nodes For the nodes to provide metrics, they need to export them via node_exporter . Unfortunately the current version does not support basic auth and TLS. Therefore use a reverse proxy and there is no excuse for not using TLS with valid certificates. Installation apt-get install prometheus-node-exporter nginx certbot python3-certbot-nginx Node Exporter To hide the node exporter to localhost and include the btrfs mounts, change /etc/default/prometheus-node-exporter ARGS=\"--web.listen-address=\\\"127.0.0.1:9100\\\" --collector.filesystem.ignored-mount-points=\\\"^/(dev|proc|run|sys|mnt|var/lib/docker|snap)($|/)\\\"\" and restart the service Reverse Proxy The nginx provides TLS and Basic Auth. Add a section to /etc/nginx/sites-enabled/default location /metrics { proxy_pass http://localhost:9100; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering off; proxy_request_buffering off; auth_basic \"Scraper\u2019s Area\"; auth_basic_user_file /etc/nginx/.htpasswd; } Set the correct server_name from _ to: server_name public.dns.tld; And provide a htpasswd formated user in /etc/nginx/.htpasswd Then run sudo certbot --nginx Prometheus To your prometheus config add a job: - job_name: 'nodeexporter' scrape_interval: 1m scheme: https basic_auth: username: scrape password: *** static_configs: - targets: - 'public.dns.tld' and reload the config. Grafana Then design a dashboard as you please.","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"The hosts for the containers are vital to the whole system. Therefore it is suggested to monitor them. This helps with placement of new containers too. A solid approach is to use the prometheus node exporter as explained below","title":"Monitoring"},{"location":"monitoring/#prometheus-grafana-stack","text":"Please deploy a Prometheus Grafana stack.","title":"Prometheus Grafana Stack"},{"location":"monitoring/#nodes","text":"For the nodes to provide metrics, they need to export them via node_exporter . Unfortunately the current version does not support basic auth and TLS. Therefore use a reverse proxy and there is no excuse for not using TLS with valid certificates.","title":"Nodes"},{"location":"monitoring/#installation","text":"apt-get install prometheus-node-exporter nginx certbot python3-certbot-nginx","title":"Installation"},{"location":"monitoring/#node-exporter","text":"To hide the node exporter to localhost and include the btrfs mounts, change /etc/default/prometheus-node-exporter ARGS=\"--web.listen-address=\\\"127.0.0.1:9100\\\" --collector.filesystem.ignored-mount-points=\\\"^/(dev|proc|run|sys|mnt|var/lib/docker|snap)($|/)\\\"\" and restart the service","title":"Node Exporter"},{"location":"monitoring/#reverse-proxy","text":"The nginx provides TLS and Basic Auth. Add a section to /etc/nginx/sites-enabled/default location /metrics { proxy_pass http://localhost:9100; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering off; proxy_request_buffering off; auth_basic \"Scraper\u2019s Area\"; auth_basic_user_file /etc/nginx/.htpasswd; } Set the correct server_name from _ to: server_name public.dns.tld; And provide a htpasswd formated user in /etc/nginx/.htpasswd Then run sudo certbot --nginx","title":"Reverse Proxy"},{"location":"monitoring/#prometheus","text":"To your prometheus config add a job: - job_name: 'nodeexporter' scrape_interval: 1m scheme: https basic_auth: username: scrape password: *** static_configs: - targets: - 'public.dns.tld' and reload the config.","title":"Prometheus"},{"location":"monitoring/#grafana","text":"Then design a dashboard as you please.","title":"Grafana"},{"location":"overview/","text":"Overview The lxd-manger builds on the following structure. Projects There are arbitrary projects which container a list of users. Container Each container belongs to a project and thereby is editable by all users of the project.","title":"Overview"},{"location":"overview/#overview","text":"The lxd-manger builds on the following structure.","title":"Overview"},{"location":"overview/#projects","text":"There are arbitrary projects which container a list of users.","title":"Projects"},{"location":"overview/#container","text":"Each container belongs to a project and thereby is editable by all users of the project.","title":"Container"}]}